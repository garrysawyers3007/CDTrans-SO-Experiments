2022-04-10 14:02:01,461 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Art_2Clipart/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Art_2_Clipart_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Art.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Clipart.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Art_2_Clipart_only_classifier/transformer_best_model.pth')
2022-04-10 14:02:01,461 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:02:01,462 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:02:01,462 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Clipart.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Art.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Art_2_Clipart_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Art_2Clipart/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Art and valid is Clipart
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     2427 |         1
  valid   |    65 |     4365 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Art_2Clipart/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Art_2Clipart/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Art_2_Clipart_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:02:15,255 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([0.4167, 1.0000, 0.5657, 0.8269, 0.3434, 0.9250, 0.5000, 0.2525, 0.3571,
        0.9192, 0.9250, 0.5960, 0.5859, 0.8750, 0.4141, 0.3265, 0.7071, 0.7179,
        0.6562, 0.8889, 0.1951, 0.8283, 0.4000, 0.2439, 0.1616, 0.5072, 0.7632,
        0.5051, 0.7980, 0.5000, 0.8372, 0.2892, 0.7321, 0.7667, 0.9000, 0.9000,
        0.4531, 0.7826, 0.8000, 0.7164, 0.7843, 0.6667, 0.5472, 0.5057, 0.7111,
        0.1500, 0.5479, 0.7980, 0.6282, 0.9348, 0.6869, 0.9000, 0.4667, 0.4444,
        0.7250, 0.8586, 0.4747, 0.9048, 0.6066, 0.4590, 0.6625, 0.6604, 0.6250,
        0.7000, 0.0282])
M2 per class acc: tensor([0.4583, 1.0000, 0.3939, 0.7885, 0.3737, 0.8750, 0.4286, 0.0404, 0.2619,
        0.8687, 0.9250, 0.7273, 0.5455, 0.5750, 0.4040, 0.2449, 0.6667, 0.6667,
        0.6562, 0.8182, 0.3415, 0.7374, 0.4250, 0.2195, 0.1717, 0.5362, 0.7105,
        0.3333, 0.6061, 0.5500, 0.7442, 0.2892, 0.3571, 0.7667, 0.8000, 0.9250,
        0.5312, 0.4348, 0.6400, 0.7761, 0.8039, 0.5867, 0.4340, 0.4943, 0.6444,
        0.1750, 0.5068, 0.6970, 0.5513, 0.9348, 0.5758, 0.8500, 0.3667, 0.3636,
        0.6500, 0.8687, 0.3737, 0.8810, 0.4098, 0.4262, 0.7000, 0.5472, 0.5500,
        0.6000, 0.0282])
Per class numbers: tensor([48., 41., 99., 52., 99., 40., 42., 99., 42., 99., 40., 99., 99., 40.,
        99., 98., 99., 39., 64., 99., 41., 99., 40., 41., 99., 69., 76., 99.,
        99., 40., 43., 83., 56., 60., 40., 40., 64., 46., 50., 67., 51., 75.,
        53., 87., 90., 40., 73., 99., 78., 46., 99., 40., 60., 99., 40., 99.,
        99., 42., 61., 61., 80., 53., 40., 40., 71.])
2022-04-10 14:03:41,218 reid_baseline.test INFO: normal accuracy 0.6061855670103092 2.2913589477539062 
2022-04-10 14:03:41,218 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:03:41,218 reid_baseline.test INFO: Accuracy: 60.6%
2022-04-10 14:03:47,513 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Art_2Product/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Art_2_Product_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Art.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Product.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Art_2_Product_only_classifier/transformer_best_model.pth')
2022-04-10 14:03:47,513 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:03:47,513 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:03:47,513 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Product.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Art.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Art_2_Product_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Art_2Product/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Art and valid is Product
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     2427 |         1
  valid   |    65 |     4439 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Art_2Product/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Art_2Product/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Art_2_Product_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:04:01,406 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([0.9552, 0.9104, 0.5323, 0.9851, 0.2604, 0.9296, 0.6939, 0.2667, 0.9565,
        0.9192, 0.8889, 0.3667, 0.9293, 0.8788, 0.6500, 0.7907, 0.9123, 0.6429,
        0.9545, 1.0000, 0.5263, 0.8780, 0.8367, 0.6506, 0.1207, 0.9222, 0.9583,
        0.6833, 0.9898, 0.7639, 0.8485, 0.5699, 0.9899, 0.7975, 0.5814, 0.8936,
        0.8065, 0.8140, 0.8276, 0.9655, 0.8714, 0.9000, 0.7849, 0.9899, 0.3232,
        0.5122, 0.9574, 0.7879, 0.9012, 1.0000, 0.9451, 0.8889, 0.9574, 0.8036,
        0.5692, 0.8788, 0.0132, 0.8400, 0.9268, 0.8837, 0.7167, 0.9268, 0.9118,
        1.0000, 0.3750])
M2 per class acc: tensor([0.9701, 0.9552, 0.4355, 0.9851, 0.3229, 0.9296, 0.6939, 0.2889, 0.9348,
        0.9293, 0.8194, 0.5111, 0.9192, 0.8990, 0.5500, 0.8837, 0.8596, 0.7619,
        0.9205, 0.9773, 0.2632, 0.8780, 0.8571, 0.7229, 0.1552, 0.8778, 0.9583,
        0.5167, 0.9898, 0.8194, 0.8687, 0.6774, 0.9495, 0.8987, 0.5349, 0.8723,
        0.8226, 0.6744, 0.7931, 0.9655, 0.9143, 0.9000, 0.9355, 0.9899, 0.3333,
        0.4634, 0.9574, 0.7576, 0.9012, 0.9877, 0.9670, 0.8519, 0.8723, 0.8393,
        0.5846, 0.8687, 0.0263, 0.7467, 0.8537, 0.7674, 0.8167, 0.9268, 0.9559,
        1.0000, 0.4286])
Per class numbers: tensor([67., 67., 62., 67., 96., 71., 49., 45., 46., 99., 72., 90., 99., 99.,
        40., 43., 57., 42., 88., 44., 38., 41., 98., 83., 58., 90., 96., 60.,
        98., 72., 99., 93., 99., 79., 43., 47., 62., 43., 58., 58., 70., 40.,
        93., 99., 99., 41., 47., 99., 81., 81., 91., 54., 47., 56., 65., 99.,
        76., 75., 41., 43., 60., 41., 68., 59., 56.])
2022-04-10 14:05:27,948 reid_baseline.test INFO: normal accuracy 0.7814823158369002 1.973944067955017 
2022-04-10 14:05:27,949 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:05:27,949 reid_baseline.test INFO: Accuracy: 78.1%
2022-04-10 14:05:34,299 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Art_2Real_World/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Art_2_Real_World_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Art.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Real_World.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Art_2_Real_World_only_classifier/transformer_best_model.pth')
2022-04-10 14:05:34,299 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:05:34,299 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:05:34,300 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Real_World.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Art.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Art_2_Real_World_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Art_2Real_World/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Art and valid is Real_World
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     2427 |         1
  valid   |    65 |     4357 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Art_2Real_World/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Art_2Real_World/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Art_2_Real_World_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:05:48,162 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([0.6471, 0.9753, 0.6667, 0.9667, 0.5781, 0.9138, 0.7727, 0.3433, 0.9351,
        0.9701, 0.8750, 0.5614, 0.9067, 0.9412, 0.8475, 0.8916, 0.9038, 0.8000,
        0.8289, 1.0000, 0.6567, 0.9483, 0.8163, 0.6774, 0.7439, 0.9167, 1.0000,
        0.8769, 0.7887, 0.8261, 0.9886, 0.5000, 0.9798, 0.8721, 0.8113, 0.7647,
        0.9062, 0.8788, 0.9500, 0.9268, 0.7667, 0.8824, 0.5432, 1.0000, 0.4568,
        0.8372, 0.9000, 0.9271, 0.9853, 0.9863, 0.9867, 0.9103, 0.9074, 0.9899,
        0.5846, 0.9091, 0.3585, 0.9863, 0.8056, 0.9048, 0.7288, 0.9759, 0.5781,
        0.9600, 0.2609])
M2 per class acc: tensor([0.6863, 0.9877, 0.5769, 0.9667, 0.5938, 0.9483, 0.7879, 0.3881, 0.9221,
        0.9552, 0.8750, 0.5789, 0.8933, 0.9412, 0.6780, 0.9157, 0.7500, 0.8471,
        0.7500, 1.0000, 0.6716, 0.9655, 0.7959, 0.7903, 0.8049, 0.9333, 1.0000,
        0.8154, 0.7465, 0.8696, 1.0000, 0.5588, 0.9798, 0.9186, 0.7925, 0.7500,
        0.8594, 0.8333, 0.9333, 0.9268, 0.7667, 0.9020, 0.7901, 1.0000, 0.4568,
        0.8837, 0.8875, 0.9167, 0.9853, 0.9589, 0.9867, 0.8333, 0.8148, 0.9798,
        0.6154, 0.9221, 0.3585, 0.8082, 0.7500, 0.8730, 0.8305, 0.9759, 0.6875,
        0.8933, 0.2609])
Per class numbers: tensor([51., 81., 78., 60., 64., 58., 66., 67., 77., 67., 72., 57., 75., 85.,
        59., 83., 52., 85., 76., 99., 67., 58., 49., 62., 82., 60., 60., 65.,
        71., 46., 88., 68., 99., 86., 53., 68., 64., 66., 60., 41., 30., 51.,
        81., 52., 81., 43., 80., 96., 68., 73., 75., 78., 54., 99., 65., 77.,
        53., 73., 36., 63., 59., 83., 64., 75., 23.])
2022-04-10 14:07:13,295 reid_baseline.test INFO: normal accuracy 0.83245352306633 1.7376307249069214 
2022-04-10 14:07:13,296 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:07:13,296 reid_baseline.test INFO: Accuracy: 83.2%
2022-04-10 14:07:19,678 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Clipart_2Art/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Clipart_2_Art_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Clipart.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Art.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Clipart_2_Art_only_classifier/transformer_best_model.pth')
2022-04-10 14:07:19,678 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:07:19,678 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:07:19,678 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Art.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Clipart.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Clipart_2_Art_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Clipart_2Art/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Clipart and valid is Art
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     4365 |         1
  valid   |    65 |     2427 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Clipart_2Art/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Clipart_2Art/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Clipart_2_Art_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:07:33,392 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([0.6000, 0.9524, 0.7071, 0.8750, 0.3636, 0.8182, 0.7857, 0.7000, 0.9024,
        0.8039, 0.9783, 0.4000, 1.0000, 0.5652, 0.5385, 0.8500, 0.3500, 0.2791,
        0.5250, 0.9867, 0.5789, 1.0000, 0.8125, 0.5217, 0.7273, 0.7089, 0.9444,
        0.8000, 0.1905, 0.6250, 0.9783, 0.7619, 0.6829, 0.9459, 0.4583, 0.4737,
        0.7778, 0.8723, 0.8667, 0.9333, 0.7368, 0.8000, 0.6190, 0.6111, 0.8000,
        0.1667, 0.7500, 0.8116, 0.7000, 0.6364, 0.9556, 0.8571, 0.6739, 0.9342,
        0.4400, 0.7500, 0.8250, 0.9750, 0.8043, 0.5750, 0.8125, 0.9167, 0.6000,
        0.7551, 0.0000])
M2 per class acc: tensor([0.3333, 0.6667, 0.4444, 0.8000, 0.3636, 0.7273, 0.6905, 0.7000, 0.8293,
        0.7059, 0.9348, 0.1000, 0.9444, 0.5652, 0.5385, 0.7500, 0.2750, 0.3023,
        0.6250, 0.9333, 0.4211, 0.9592, 0.8750, 0.5217, 0.7955, 0.5570, 0.8889,
        0.7000, 0.1667, 0.3750, 0.9783, 0.6190, 0.6585, 0.9324, 0.5833, 0.5263,
        0.4815, 0.8723, 0.7333, 0.8667, 0.6842, 0.8000, 0.5238, 0.5000, 0.6500,
        0.1667, 0.7000, 0.7101, 0.7500, 0.4545, 0.9556, 0.6531, 0.7826, 0.9079,
        0.4800, 0.7500, 0.8500, 0.9000, 0.5435, 0.6500, 0.8125, 0.9167, 0.5000,
        0.7143, 0.0000])
Per class numbers: tensor([15., 21., 99., 40., 44., 22., 42., 20., 41., 51., 46., 20., 18., 46.,
        26., 40., 40., 43., 40., 75., 19., 49., 16., 23., 44., 79., 18., 20.,
        42., 32., 46., 21., 41., 74., 24., 19., 27., 47., 45., 15., 19., 30.,
        21., 18., 20., 18., 40., 69., 20., 33., 90., 49., 46., 76., 25., 20.,
        40., 40., 46., 40., 16., 72., 20., 49., 20.])
2022-04-10 14:08:30,960 reid_baseline.test INFO: normal accuracy 0.7437165224557066 2.3550140857696533 
2022-04-10 14:08:30,960 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:08:30,960 reid_baseline.test INFO: Accuracy: 74.4%
2022-04-10 14:08:37,324 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Clipart_2Product/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Clipart_2_Product_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Clipart.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Product.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Clipart_2_Product_only_classifier/transformer_best_model.pth')
2022-04-10 14:08:37,324 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:08:37,324 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:08:37,324 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Product.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Clipart.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Clipart_2_Product_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Clipart_2Product/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Clipart and valid is Product
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     4365 |         1
  valid   |    65 |     4439 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Clipart_2Product/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Clipart_2Product/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Clipart_2_Product_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:08:51,118 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([1.0000, 0.8358, 0.6935, 0.9254, 0.6875, 0.8592, 0.6939, 0.6222, 0.9565,
        0.8081, 0.8750, 0.2333, 0.9899, 0.2121, 0.6500, 0.7674, 0.9474, 0.1905,
        0.9318, 1.0000, 0.2895, 0.9024, 0.7959, 0.6627, 0.8793, 0.9333, 0.9271,
        0.7500, 0.9694, 0.7361, 0.8889, 0.8280, 0.9798, 0.3544, 0.1395, 0.7234,
        0.6935, 0.8140, 0.7931, 0.9828, 0.9429, 0.9500, 0.8495, 0.9899, 0.5556,
        0.4390, 0.8936, 0.8788, 0.8889, 0.8765, 0.9780, 0.7222, 0.9574, 0.7857,
        0.3231, 0.9596, 0.3289, 0.8800, 0.8537, 0.7209, 0.9167, 0.8537, 1.0000,
        1.0000, 0.1607])
M2 per class acc: tensor([0.9851, 0.5672, 0.4839, 0.9403, 0.6875, 0.8873, 0.6327, 0.6000, 0.9565,
        0.8485, 0.9306, 0.1556, 1.0000, 0.2525, 0.6500, 0.7674, 0.9123, 0.3095,
        0.9318, 0.9773, 0.1842, 0.8780, 0.8469, 0.7108, 0.9310, 0.9000, 0.9062,
        0.7167, 0.8265, 0.5000, 0.8788, 0.8495, 0.9495, 0.3797, 0.1860, 0.6170,
        0.3226, 0.8372, 0.8276, 0.9828, 0.9143, 0.9000, 0.8065, 0.9899, 0.5657,
        0.3415, 0.8723, 0.8485, 0.8642, 0.7284, 0.9560, 0.7963, 0.9149, 0.7321,
        0.3538, 0.9192, 0.5526, 0.7200, 0.7317, 0.7907, 0.9000, 0.8780, 0.9706,
        1.0000, 0.3393])
Per class numbers: tensor([67., 67., 62., 67., 96., 71., 49., 45., 46., 99., 72., 90., 99., 99.,
        40., 43., 57., 42., 88., 44., 38., 41., 98., 83., 58., 90., 96., 60.,
        98., 72., 99., 93., 99., 79., 43., 47., 62., 43., 58., 58., 70., 40.,
        93., 99., 99., 41., 47., 99., 81., 81., 91., 54., 47., 56., 65., 99.,
        76., 75., 41., 43., 60., 41., 68., 59., 56.])
2022-04-10 14:10:17,728 reid_baseline.test INFO: normal accuracy 0.7753998648344221 2.0048115253448486 
2022-04-10 14:10:17,728 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:10:17,728 reid_baseline.test INFO: Accuracy: 77.5%
2022-04-10 14:10:24,837 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Clipart_2Real_World/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Clipart_2_Real_World_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Clipart.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Real_World.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Clipart_2_Real_World_only_classifier/transformer_best_model.pth')
2022-04-10 14:10:24,838 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:10:24,838 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:10:24,838 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Real_World.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Clipart.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Clipart_2_Real_World_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Clipart_2Real_World/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Clipart and valid is Real_World
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     4365 |         1
  valid   |    65 |     4357 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Clipart_2Real_World/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Clipart_2Real_World/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Clipart_2_Real_World_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:10:38,804 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([0.6863, 0.9012, 0.6795, 0.9667, 0.4219, 0.9483, 0.7121, 0.6716, 0.8312,
        0.9403, 0.9028, 0.6667, 0.9600, 0.7176, 0.7797, 0.8072, 0.9615, 0.3176,
        0.8289, 0.9899, 0.3881, 0.9828, 0.8776, 0.7258, 0.8902, 0.9833, 0.9500,
        0.9077, 0.6338, 0.6739, 0.9886, 0.8235, 0.9798, 0.8256, 0.4906, 0.5294,
        0.5625, 0.9242, 0.9500, 0.9512, 0.8000, 0.8431, 0.7901, 1.0000, 0.6790,
        0.5814, 0.7875, 0.9479, 0.9412, 0.9041, 0.9733, 0.8205, 0.7963, 0.9899,
        0.2923, 0.9870, 0.6415, 0.9726, 0.8333, 0.8889, 0.8814, 0.9639, 0.6719,
        0.8800, 0.3043])
M2 per class acc: tensor([0.7451, 0.8272, 0.5000, 0.9667, 0.3750, 0.9310, 0.5606, 0.6119, 0.8312,
        0.9104, 0.9444, 0.4561, 0.9733, 0.7059, 0.7627, 0.7590, 0.9231, 0.2824,
        0.8289, 0.9899, 0.2985, 0.9483, 0.8163, 0.7419, 0.8780, 0.9833, 0.9500,
        0.8462, 0.5211, 0.4565, 0.9773, 0.8235, 0.9596, 0.7558, 0.5849, 0.5735,
        0.4062, 0.8636, 0.8833, 0.9512, 0.7333, 0.8627, 0.6296, 0.9808, 0.6296,
        0.7442, 0.7875, 0.8750, 0.9853, 0.7260, 0.9200, 0.7821, 0.8148, 0.9899,
        0.1385, 0.9351, 0.6226, 0.8767, 0.7222, 0.8889, 0.8644, 0.9277, 0.6406,
        0.8933, 0.1739])
Per class numbers: tensor([51., 81., 78., 60., 64., 58., 66., 67., 77., 67., 72., 57., 75., 85.,
        59., 83., 52., 85., 76., 99., 67., 58., 49., 62., 82., 60., 60., 65.,
        71., 46., 88., 68., 99., 86., 53., 68., 64., 66., 60., 41., 30., 51.,
        81., 52., 81., 43., 80., 96., 68., 73., 75., 78., 54., 99., 65., 77.,
        53., 73., 36., 63., 59., 83., 64., 75., 23.])
2022-04-10 14:12:05,176 reid_baseline.test INFO: normal accuracy 0.8095019508836355 2.103026866912842 
2022-04-10 14:12:05,177 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:12:05,177 reid_baseline.test INFO: Accuracy: 81.0%
2022-04-10 14:12:11,817 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Product_2Art/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Product_2_Art_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Product.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Art.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Product_2_Art_only_classifier/transformer_best_model.pth')
2022-04-10 14:12:11,817 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:12:11,818 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:12:11,818 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Art.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Product.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Product_2_Art_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Product_2Art/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Product and valid is Art
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     4439 |         1
  valid   |    65 |     2427 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Product_2Art/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Product_2Art/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Product_2_Art_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:12:25,632 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([0.6000, 0.9524, 0.6465, 0.8250, 0.2727, 1.0000, 0.5714, 0.8500, 0.7073,
        0.5686, 0.8478, 0.4500, 0.9444, 0.9783, 0.6923, 0.7750, 0.2250, 0.3721,
        0.5750, 0.9200, 0.1053, 1.0000, 0.8750, 0.6522, 0.7727, 0.6456, 0.8889,
        0.7000, 0.0952, 0.5625, 0.9348, 0.5714, 0.7317, 0.8649, 0.8333, 0.8421,
        0.7037, 0.8936, 0.6000, 0.9333, 0.7895, 0.8667, 0.6190, 0.5556, 0.7000,
        0.3333, 0.7750, 0.6232, 0.9500, 0.7576, 0.9444, 0.8367, 0.6522, 0.9342,
        0.5200, 0.9000, 0.7500, 0.9000, 0.8696, 0.6750, 0.5000, 0.9444, 0.5500,
        0.6939, 0.3500])
M2 per class acc: tensor([0.2667, 0.9048, 0.4949, 0.7250, 0.2045, 0.9091, 0.5714, 0.7000, 0.7073,
        0.5686, 0.8043, 0.4500, 0.9444, 0.9783, 0.6923, 0.7000, 0.2000, 0.3023,
        0.5000, 0.7467, 0.0000, 0.9796, 0.8750, 0.6522, 0.7273, 0.5190, 0.7778,
        0.7500, 0.1429, 0.4375, 0.8696, 0.5238, 0.7073, 0.8649, 0.8333, 0.7368,
        0.5185, 0.8511, 0.6444, 0.8667, 0.8421, 0.8667, 0.6667, 0.6111, 0.7000,
        0.3889, 0.6250, 0.6232, 0.8500, 0.7273, 0.9556, 0.7143, 0.6522, 0.9211,
        0.6000, 0.8000, 0.6250, 0.8250, 0.8043, 0.7000, 0.3750, 0.9583, 0.3500,
        0.7143, 0.2000])
Per class numbers: tensor([15., 21., 99., 40., 44., 22., 42., 20., 41., 51., 46., 20., 18., 46.,
        26., 40., 40., 43., 40., 75., 19., 49., 16., 23., 44., 79., 18., 20.,
        42., 32., 46., 21., 41., 74., 24., 19., 27., 47., 45., 15., 19., 30.,
        21., 18., 20., 18., 40., 69., 20., 33., 90., 49., 46., 76., 25., 20.,
        40., 40., 46., 40., 16., 72., 20., 49., 20.])
2022-04-10 14:13:23,549 reid_baseline.test INFO: normal accuracy 0.7251751133086115 2.143260955810547 
2022-04-10 14:13:23,549 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:13:23,549 reid_baseline.test INFO: Accuracy: 72.5%
2022-04-10 14:13:29,702 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Product_2Clipart/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Product_2_Clipart_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Product.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Clipart.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Product_2_Clipart_only_classifier/transformer_best_model.pth')
2022-04-10 14:13:29,702 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:13:29,702 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:13:29,702 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Clipart.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Product.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Product_2_Clipart_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Product_2Clipart/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Product and valid is Clipart
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     4439 |         1
  valid   |    65 |     4365 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Product_2Clipart/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Product_2Clipart/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Product_2_Clipart_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:13:43,877 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([0.4792, 0.9756, 0.4545, 0.8269, 0.2626, 0.9250, 0.2143, 0.5758, 0.2619,
        0.7576, 0.8000, 0.4040, 0.5354, 0.8500, 0.4141, 0.2347, 0.6869, 0.0769,
        0.5625, 0.7778, 0.0976, 0.8081, 0.6250, 0.3659, 0.2727, 0.7101, 0.6316,
        0.5859, 0.6566, 0.6000, 0.7674, 0.4578, 0.5179, 0.6333, 0.2750, 0.8250,
        0.2812, 0.5000, 0.5000, 0.8657, 0.7255, 0.6400, 0.5849, 0.3333, 0.6889,
        0.0500, 0.4110, 0.7475, 0.7308, 0.9130, 0.5051, 0.7750, 0.4167, 0.3636,
        0.3500, 0.9192, 0.2525, 0.6190, 0.5410, 0.3770, 0.6750, 0.7736, 0.5250,
        0.3750, 0.2535])
M2 per class acc: tensor([0.5000, 0.9512, 0.4343, 0.6923, 0.3333, 0.8750, 0.1905, 0.6061, 0.1667,
        0.8182, 0.8750, 0.3838, 0.5051, 0.8000, 0.4848, 0.2857, 0.6869, 0.0513,
        0.4688, 0.7677, 0.0732, 0.8081, 0.5250, 0.4878, 0.2121, 0.7536, 0.7895,
        0.6162, 0.6667, 0.5500, 0.6744, 0.5542, 0.5179, 0.6833, 0.0750, 0.8500,
        0.1250, 0.3478, 0.4600, 0.7761, 0.7255, 0.5867, 0.6792, 0.4598, 0.6000,
        0.0750, 0.3425, 0.7172, 0.7564, 0.8913, 0.4444, 0.5750, 0.5000, 0.3333,
        0.4750, 0.8889, 0.1818, 0.6190, 0.4590, 0.3443, 0.6625, 0.6981, 0.4500,
        0.4250, 0.2254])
Per class numbers: tensor([48., 41., 99., 52., 99., 40., 42., 99., 42., 99., 40., 99., 99., 40.,
        99., 98., 99., 39., 64., 99., 41., 99., 40., 41., 99., 69., 76., 99.,
        99., 40., 43., 83., 56., 60., 40., 40., 64., 46., 50., 67., 51., 75.,
        53., 87., 90., 40., 73., 99., 78., 46., 99., 40., 60., 99., 40., 99.,
        99., 42., 61., 61., 80., 53., 40., 40., 71.])
2022-04-10 14:15:08,438 reid_baseline.test INFO: normal accuracy 0.5473081328751432 2.3075907230377197 
2022-04-10 14:15:08,438 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:15:08,438 reid_baseline.test INFO: Accuracy: 54.7%
2022-04-10 14:15:14,747 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Product_2Real_World/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Product_2_Real_World_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Product.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Real_World.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Product_2_Real_World_only_classifier/transformer_best_model.pth')
2022-04-10 14:15:14,748 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:15:14,748 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:15:14,748 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Real_World.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Product.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Product_2_Real_World_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Product_2Real_World/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Product and valid is Real_World
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     4439 |         1
  valid   |    65 |     4357 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Product_2Real_World/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Product_2Real_World/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Product_2_Real_World_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:15:28,908 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([0.6667, 0.9753, 0.3462, 0.9500, 0.5000, 0.9828, 0.7273, 0.7015, 0.8701,
        0.8358, 0.9722, 0.5965, 0.9867, 0.9647, 0.8814, 0.8675, 0.9231, 0.5294,
        0.7500, 0.9798, 0.2836, 0.9655, 0.9184, 0.8710, 0.8902, 0.9667, 0.9500,
        0.8769, 0.5493, 0.9783, 0.9773, 0.5588, 0.9899, 0.9767, 0.7925, 0.8235,
        0.7188, 0.8182, 0.8167, 0.9512, 0.8000, 0.9216, 0.8519, 1.0000, 0.8395,
        0.6977, 0.8625, 0.8021, 1.0000, 0.9589, 1.0000, 0.9103, 0.8519, 1.0000,
        0.5538, 0.9870, 0.6604, 0.9726, 0.7778, 0.9048, 0.8305, 0.9518, 0.6875,
        0.9333, 0.5217])
M2 per class acc: tensor([0.6863, 0.9753, 0.2949, 0.8667, 0.3125, 1.0000, 0.6667, 0.5373, 0.8961,
        0.8806, 0.9583, 0.6667, 0.9733, 0.9765, 0.8475, 0.8193, 0.9231, 0.4941,
        0.7632, 0.9697, 0.1791, 0.9655, 0.8163, 0.9355, 0.8902, 0.9500, 0.9500,
        0.8462, 0.5775, 1.0000, 0.9886, 0.5735, 0.9899, 0.9535, 0.9245, 0.7794,
        0.6719, 0.8182, 0.8667, 0.9024, 0.8000, 0.9020, 0.8519, 1.0000, 0.9012,
        0.6744, 0.8375, 0.8646, 0.9853, 0.9726, 1.0000, 0.7692, 0.7222, 0.9899,
        0.3846, 0.9610, 0.5472, 0.9589, 0.7500, 0.8889, 0.7288, 0.9518, 0.6875,
        0.9067, 0.6087])
Per class numbers: tensor([51., 81., 78., 60., 64., 58., 66., 67., 77., 67., 72., 57., 75., 85.,
        59., 83., 52., 85., 76., 99., 67., 58., 49., 62., 82., 60., 60., 65.,
        71., 46., 88., 68., 99., 86., 53., 68., 64., 66., 60., 41., 30., 51.,
        81., 52., 81., 43., 80., 96., 68., 73., 75., 78., 54., 99., 65., 77.,
        53., 73., 36., 63., 59., 83., 64., 75., 23.])
2022-04-10 14:16:56,312 reid_baseline.test INFO: normal accuracy 0.8381914161120037 1.6148772239685059 
2022-04-10 14:16:56,312 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:16:56,312 reid_baseline.test INFO: Accuracy: 83.8%
2022-04-10 14:17:03,103 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Real_World_2Art/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Real_World_2_Art_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Real_World.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Art.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Real_World_2_Art_only_classifier/transformer_best_model.pth')
2022-04-10 14:17:03,103 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:17:03,103 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:17:03,103 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Art.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Real_World.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Real_World_2_Art_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Real_World_2Art/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Real_World and valid is Art
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     4357 |         1
  valid   |    65 |     2427 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Real_World_2Art/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Real_World_2Art/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Real_World_2_Art_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:17:17,106 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([0.5333, 1.0000, 0.7273, 0.9500, 0.4545, 0.8182, 0.5714, 1.0000, 0.8293,
        0.6078, 0.9348, 0.6500, 0.8889, 1.0000, 0.5385, 0.8000, 0.3750, 0.6744,
        0.8000, 0.9600, 0.8947, 1.0000, 0.8125, 0.5217, 0.7955, 0.8608, 0.8889,
        0.7500, 0.3095, 0.5000, 0.9783, 0.8095, 0.7317, 0.9730, 0.9167, 0.9474,
        0.7407, 0.8511, 0.8667, 0.8667, 0.4737, 0.8667, 0.4286, 0.6667, 0.6500,
        0.5556, 0.8500, 0.6667, 1.0000, 0.7273, 0.9111, 0.8776, 0.8478, 0.9079,
        0.5600, 0.7000, 0.7750, 0.9250, 0.7826, 0.6000, 0.6250, 0.9028, 0.5500,
        0.7143, 0.2000])
M2 per class acc: tensor([0.4000, 1.0000, 0.6667, 0.9000, 0.5000, 0.8636, 0.5238, 0.9500, 0.8049,
        0.6275, 0.9348, 0.7000, 0.8333, 1.0000, 0.6538, 0.7500, 0.2750, 0.6744,
        0.8750, 0.9467, 0.9474, 0.9796, 0.9375, 0.4783, 0.7955, 0.8608, 0.8889,
        0.7500, 0.2857, 0.4062, 0.9348, 0.5238, 0.5610, 0.9595, 0.8750, 0.8947,
        0.7778, 0.9362, 0.8222, 0.8000, 0.7368, 0.8000, 0.4762, 0.7222, 0.6500,
        0.4444, 0.8500, 0.6232, 0.9500, 0.7576, 0.8889, 0.7959, 0.7391, 0.9079,
        0.6000, 0.7000, 0.7500, 0.9750, 0.7609, 0.4750, 0.5000, 0.9028, 0.4000,
        0.6939, 0.0500])
Per class numbers: tensor([15., 21., 99., 40., 44., 22., 42., 20., 41., 51., 46., 20., 18., 46.,
        26., 40., 40., 43., 40., 75., 19., 49., 16., 23., 44., 79., 18., 20.,
        42., 32., 46., 21., 41., 74., 24., 19., 27., 47., 45., 15., 19., 30.,
        21., 18., 20., 18., 40., 69., 20., 33., 90., 49., 46., 76., 25., 20.,
        40., 40., 46., 40., 16., 72., 20., 49., 20.])
2022-04-10 14:18:14,961 reid_baseline.test INFO: normal accuracy 0.7766790276060981 1.4139467477798462 
2022-04-10 14:18:14,962 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:18:14,962 reid_baseline.test INFO: Accuracy: 77.7%
2022-04-10 14:18:21,028 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Real_World_2Clipart/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Real_World_2_Clipart_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Real_World.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Clipart.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Real_World_2_Clipart_only_classifier/transformer_best_model.pth')
2022-04-10 14:18:21,028 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:18:21,028 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:18:21,029 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Clipart.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Real_World.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Real_World_2_Clipart_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Real_World_2Clipart/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Real_World and valid is Clipart
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     4357 |         1
  valid   |    65 |     4365 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Real_World_2Clipart/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Real_World_2Clipart/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Real_World_2_Clipart_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:18:34,676 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([0.5833, 1.0000, 0.5960, 0.8654, 0.5556, 0.8250, 0.3571, 0.4040, 0.2143,
        0.5354, 0.9000, 0.7677, 0.4747, 0.8750, 0.3939, 0.3776, 0.7778, 0.2564,
        0.3594, 0.8182, 0.3902, 0.8485, 0.6500, 0.2683, 0.2020, 0.7826, 0.6711,
        0.4444, 0.5758, 0.4750, 0.7674, 0.5301, 0.6607, 0.7667, 0.8000, 0.8250,
        0.3438, 0.5652, 0.7200, 0.8209, 0.5686, 0.5467, 0.3585, 0.4023, 0.7889,
        0.1000, 0.4521, 0.8586, 0.7436, 0.9130, 0.3434, 0.9000, 0.4167, 0.3131,
        0.1250, 0.8283, 0.3737, 0.8333, 0.6393, 0.2131, 0.6750, 0.6792, 0.5500,
        0.3500, 0.4085])
M2 per class acc: tensor([0.5833, 0.9512, 0.5657, 0.7692, 0.5758, 0.8250, 0.3333, 0.4040, 0.2619,
        0.3737, 0.9000, 0.8889, 0.5152, 0.9000, 0.4343, 0.3367, 0.7576, 0.2051,
        0.3750, 0.7778, 0.3902, 0.7879, 0.6000, 0.1707, 0.2121, 0.7536, 0.6711,
        0.4040, 0.6061, 0.5000, 0.7907, 0.4337, 0.4821, 0.7833, 0.8250, 0.9000,
        0.3438, 0.3261, 0.6400, 0.7910, 0.6667, 0.5200, 0.4340, 0.4483, 0.7444,
        0.1000, 0.3151, 0.8081, 0.7308, 0.8261, 0.2121, 0.8500, 0.3833, 0.2727,
        0.0250, 0.8889, 0.3737, 0.7619, 0.6393, 0.2459, 0.6500, 0.5849, 0.6000,
        0.3750, 0.4085])
Per class numbers: tensor([48., 41., 99., 52., 99., 40., 42., 99., 42., 99., 40., 99., 99., 40.,
        99., 98., 99., 39., 64., 99., 41., 99., 40., 41., 99., 69., 76., 99.,
        99., 40., 43., 83., 56., 60., 40., 40., 64., 46., 50., 67., 51., 75.,
        53., 87., 90., 40., 73., 99., 78., 46., 99., 40., 60., 99., 40., 99.,
        99., 42., 61., 61., 80., 53., 40., 40., 71.])
2022-04-10 14:19:59,103 reid_baseline.test INFO: normal accuracy 0.5713631156930126 1.8427932262420654 
2022-04-10 14:19:59,103 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:19:59,103 reid_baseline.test INFO: Accuracy: 57.1%
2022-04-10 14:20:05,556 reid_baseline INFO: Namespace(config_file='configs/pretrain.yml', opts=['MODEL.DEVICE_ID', "('0')", 'TEST.WEIGHT', "('../logs/pretrain/deit_base/office-home/Real_World_2Product/transformer_best_model.pth')", 'DATASETS.NAMES', 'OfficeHome', 'OUTPUT_DIR', '../logs/pretrain/deit_base/office-home/Real_World_2_Product_only_classifier', 'DATASETS.ROOT_TRAIN_DIR', './data/OfficeHomeDataset/Real_World.txt', 'DATASETS.ROOT_TEST_DIR', './data/OfficeHomeDataset/Product.txt', 'TEST.IMS_PER_BATCH', '32'], aug_type=None, alpha=0.1, feature_model_path=None, num_patch_wise=0, layer_num=0, per_class_acc=True, imgnet_model_path='../logs/pretrain/deit_base/office-home/Real_World_2_Product_only_classifier/transformer_best_model.pth')
2022-04-10 14:20:05,556 reid_baseline INFO: Loaded configuration file configs/pretrain.yml
2022-04-10 14:20:05,556 reid_baseline INFO: 
MODEL:
  PRETRAIN_CHOICE: 'imagenet'
  PRETRAIN_PATH: './data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth'
  METRIC_LOSS_TYPE: 'triplet'
  IF_LABELSMOOTH: 'on'
  IF_WITH_CENTER: 'no'
  NAME: 'transformer'
  NO_MARGIN: True
  DEVICE_ID: ('0')
  Transformer_TYPE: 'vit_base_patch16_224_TransReID' #vit_small_patch16_224_TransReID
  STRIDE_SIZE: [16, 16]
  FC_SETTING: 'TransReID'
  TASK_TYPE: 'classify_DA' 
  UDA_STAGE: 'pretrain'

INPUT:
  SIZE_TRAIN: [256, 256]
  SIZE_TEST: [256, 256]
  SIZE_CROP: [224, 224]
  PROB: 0.5 # random horizontal flip
  RE_PROB: 0.0 # random erasing
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]

DATASETS:
  NAMES: ('OURAPI')
  ROOT_TRAIN_DIR: ('./datasets/reid_datasets/Corrected_Market1501')
  NAMES2: ('OURAPI')
  ROOT_TRAIN_DIR2: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')
  ROOT_TEST_DIR: ('./datasets/reid_datasets/Refined_DukeMTMC_reID')

DATALOADER:
  SAMPLER: 'softmax'
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

SOLVER:
  OPTIMIZER_NAME: 'SGD'
  MAX_EPOCHS: 20
  BASE_LR: 0.008
  IMS_PER_BATCH: 16
  STEPS: [40, 80]
  GAMMA: 0.
  WARMUP_FACTOR: 0.01
  WARMUP_EPOCHS: 10
  WARMUP_METHOD: 'linear'
  LARGE_FC_LR: False
  CHECKPOINT_PERIOD: 10
  LOG_PERIOD: 100
  EVAL_PERIOD: 1
  WEIGHT_DECAY:  1e-4
  WEIGHT_DECAY_BIAS: 1e-4
  BIAS_LR_FACTOR: 2

TEST:
  EVAL: True
  IMS_PER_BATCH: 64
  RE_RANKING: False
  RE_RANKING_TRACK: False

  WEIGHT: '../logs/trans4DA/transformer_120.pth'
  NECK_FEAT: 'after'
  FEAT_NORM: 'yes'

OUTPUT_DIR: '../logs/trans4DA'



2022-04-10 14:20:05,556 reid_baseline INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 8
  SAMPLER: softmax
DATASETS:
  NAMES: OfficeHome
  NAMES2: OURAPI
  PLUS_NUM_ID: 100
  QUERY_MINING: False
  ROOT_TEST_DIR: ./data/OfficeHomeDataset/Product.txt
  ROOT_TRAIN_DIR: ./data/OfficeHomeDataset/Real_World.txt
  ROOT_TRAIN_DIR2: ./datasets/reid_datasets/Refined_DukeMTMC_reID
INPUT:
  AA_PROB: 0.0
  PADDING: 0
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  PROB: 0.5
  RE_PROB: 0.0
  SIZE_CROP: [224, 224]
  SIZE_TEST: [256, 256]
  SIZE_TRAIN: [256, 256]
MODEL:
  AIE_COE: 1.5
  BLOCK_PATTERN: normal
  CAMERA_EMBEDDING: False
  COS_LAYER: False
  DEVICE: cuda
  DEVICE_ID: 0
  DEVIDE_LENGTH: 4
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FC_SETTING: TransReID
  FROZEN: -1
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 1.0
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LAST_STRIDE: 1
  LOCAL_F: False
  METRIC_LOSS_TYPE: triplet
  NAME: transformer
  NECK: bnneck
  NO_MARGIN: True
  NO_SHUFFLE: False
  PATCH_SHUFFLE: 2
  PRETRAIN_CHOICE: imagenet
  PRETRAIN_PATH: ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
  PROB: 0.0
  RECIP_LOSS: 
  STRIDE_SIZE: [16, 16]
  TASK_TYPE: classify_DA
  THRESH: 0.23
  TRIPLET_LOSS_WEIGHT: 1.0
  Transformer_TYPE: vit_base_patch16_224_TransReID
  UDA_STAGE: pretrain
  VIEWPOINT_EMBEDDING: False
  YIWEI_NUM: 5
  lameda: 0.5
OUTPUT_DIR: ../logs/pretrain/deit_base/office-home/Real_World_2_Product_only_classifier
SOLVER:
  BASE_LR: 0.008
  BIAS_LR_FACTOR: 2
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.0
  IMS_PER_BATCH: 16
  LARGE_FC_LR: False
  LOG_PERIOD: 100
  MARGIN: 0.3
  MAX_EPOCHS: 20
  MOMENTUM: 0.9
  OPTIMIZER_NAME: SGD
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 80)
  WARMUP_EPOCHS: 10
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WITH_PSEUDO_LABEL_FILTER: False
TEST:
  DIST_MAT: dist_mat.npy
  EVAL: True
  FEAT_NORM: yes
  FLIP_FEATS: off
  IMS_PER_BATCH: 32
  NECK_FEAT: after
  RE_RANKING: False
  RE_RANKING_TRACK: False
  WEIGHT: ../logs/pretrain/deit_base/office-home/Real_World_2Product/transformer_best_model.pth
=> Office-Home loaded
Dataset statistics:
1 1
train Real_World and valid is Product
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train   |    65 |     4357 |         1
  valid   |    65 |     4439 |         1
  ----------------------------------------
use shuffle sampler strategy
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Real_World_2Product/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Real_World_2Product/transformer_best_model.pth
using Transformer_type: vit_base_patch16_224_TransReID as a backbone
using stride: [16, 16], and part number is num_y14 * num_x14
using drop_path_rate is : 0.1
using aie_xishu is : 1.5
embed_diim 768 mlp_ratio 4
distill need to choose right cls token in the pth
Resized position embedding: %s to %s torch.Size([1, 197, 768]) torch.Size([1, 197, 768])
Position embedding resize to height:14 width: 14
Loading pretrained ImageNet model......from ./data/pretrainModel/deit_base_distilled_patch16_224-df68dfff.pth
===========building transformer===========
Loading pretrained model for finetuning from ../logs/pretrain/deit_base/office-home/Real_World_2_Product_only_classifier/transformer_best_model.pth
Per class acc
2022-04-10 14:20:19,349 reid_baseline.test INFO: Enter inferencing
Using 4 GPUs for inference
M1 per class acc: tensor([1.0000, 0.9851, 0.6129, 0.9701, 0.3021, 0.9155, 0.7755, 0.7333, 0.9565,
        0.7879, 0.9861, 0.6222, 0.9798, 0.9091, 0.7000, 0.9302, 0.9474, 0.9524,
        0.9091, 0.9773, 0.8947, 0.8780, 0.9388, 0.9398, 0.5690, 0.9444, 0.9375,
        0.6167, 0.9184, 0.9167, 0.8990, 0.8387, 0.9798, 0.9873, 0.5349, 1.0000,
        0.8871, 0.7209, 0.7931, 0.9310, 0.8429, 0.9000, 0.8387, 0.9798, 0.8990,
        0.5366, 0.9149, 0.8788, 0.8889, 0.9877, 0.8791, 0.9259, 0.9149, 0.7857,
        0.6154, 0.9495, 0.5395, 0.9733, 0.9756, 0.7442, 0.7833, 0.9268, 1.0000,
        0.9831, 0.5714])
M2 per class acc: tensor([0.9701, 0.9403, 0.5968, 0.9851, 0.5417, 0.9437, 0.7959, 0.7556, 0.9783,
        0.7576, 0.9861, 0.5889, 0.9798, 0.9192, 0.5500, 0.9070, 0.9474, 0.9524,
        0.9318, 0.9545, 0.9474, 0.8780, 0.9388, 0.9157, 0.5345, 0.9222, 0.9167,
        0.6167, 0.9388, 0.9583, 0.8889, 0.8065, 0.9697, 0.9747, 0.5581, 0.9574,
        0.8871, 0.7442, 0.7931, 0.9655, 0.8143, 0.9000, 0.8925, 0.9899, 0.8990,
        0.4878, 0.9362, 0.8485, 0.8642, 0.9877, 0.8022, 0.8889, 0.8298, 0.8214,
        0.5538, 0.9394, 0.5658, 0.9600, 0.9024, 0.6279, 0.8167, 0.8780, 0.9853,
        0.9831, 0.6071])
Per class numbers: tensor([67., 67., 62., 67., 96., 71., 49., 45., 46., 99., 72., 90., 99., 99.,
        40., 43., 57., 42., 88., 44., 38., 41., 98., 83., 58., 90., 96., 60.,
        98., 72., 99., 93., 99., 79., 43., 47., 62., 43., 58., 58., 70., 40.,
        93., 99., 99., 41., 47., 99., 81., 81., 91., 54., 47., 56., 65., 99.,
        76., 75., 41., 43., 60., 41., 68., 59., 56.])
2022-04-10 14:21:45,796 reid_baseline.test INFO: normal accuracy 0.8569497634602388 1.119141697883606 
2022-04-10 14:21:45,797 reid_baseline.test INFO: Classify Domain Adapatation Validation Results - In the source trained model
2022-04-10 14:21:45,797 reid_baseline.test INFO: Accuracy: 85.7%
